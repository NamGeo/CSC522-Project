# -*- coding: utf-8 -*-
"""ALDA_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1not_-PnXct7aUoY20OFqHOUQpzJhuuoa
"""

#Mount drive to allow access to data files
from google.colab import drive
drive.mount('/content/drive')



"""# Data Preprocessing"""

#Modify path to reflect location of data files in your Google Drive
path = "/content/drive/My Drive/alda/"

import numpy as np
import pandas as pd

from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn import metrics
import keras 
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
from numpy.random import seed

import lightgbm as lgb

from sklearn.ensemble import RandomForestClassifier, forest

from sklearn.tree import DecisionTreeClassifier

"""###Preprocessing data to reduce feature count

###Load Data
"""

train_df = pd.read_csv(path + 'train.csv')
test_df = pd.read_csv(path + 'test.csv')
(train_df.isnull().sum()/train_df.shape[0]).sort_values(ascending=False)

#Check skewness of prediction class variable
sum(train_df['HasDetections'].to_list())

#Function for processing
def processing(df): 
  columns = []
  sk_df = pd.DataFrame([{'column': c, 'uniq': df[c].nunique(), 'skewness': df[c].value_counts(normalize=True).values[0] * 100} for c in df.columns])
  sk_df = sk_df.sort_values('skewness', ascending=False)
  columns.extend(sk_df[sk_df.skewness > 90].column.tolist())
  return columns

#Add removable attributes to list of removable attributes.
removable_attributes = processing(train_df)
removable_attributes.append('PuaMode')
removable_attributes.append('Census_ProcessorClass')
removable_attributes.append('DefaultBrowsersIdentifier')


removable_attributes = list(set(removable_attributes))


#Drop removable attributes
train_df.drop(removable_attributes, axis=1, inplace=True)
test_df.drop(removable_attributes, axis=1, inplace=True)

"""Standardization"""

pca_train = train_df.select_dtypes(['float64','int64'])
metrics = {}
for attribute in list(pca_train.columns):
  if attribute != "HasDetections":
    metrics[attribute] = {}
    metrics[attribute]["mean"] = train_df[attribute].mean()
    metrics[attribute]["std"] = train_df[attribute].std()
    train_df[attribute] = train_df[attribute].fillna(metrics[attribute]["mean"])
    test_df[attribute] = test_df[attribute].fillna(metrics[attribute]["mean"])
    
    train_df[attribute] = (train_df[attribute] - metrics[attribute]["mean"]) / (metrics[attribute]["std"])
    test_df[attribute] = (test_df[attribute] - metrics[attribute]["mean"]) / (metrics[attribute]["std"])

"""###Neural network"""

x_train = train_df.select_dtypes(['float64','int64'])
x_train.drop(['HasDetections'], axis=1, inplace=True)
y_train = train_df['HasDetections']

x_test = test_df.select_dtypes(['float64','int64'])
x_test.drop(['HasDetections'], axis=1, inplace=True)
y_test = test_df['HasDetections']

x_train.shape

def getModel(hid_n): 
  model = Sequential()
  model.add(Dense(hid_n, input_dim=33, activation='relu'))
  model.add(Dense(30, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
  return model

n_list = [50]
for n in n_list:
  print(f'N  = {n}')
  m = getModel(n)
  history = m.fit(x_train, y_train, epochs=8, batch_size=300)

prediction1 = m.predict_classes(x_test)

confusion_matrix(y_test.values, prediction1)

print(classification_report(y_test,prediction1))

"""End of Neural net"""

#bins = [-np.inf , -1.282, -0.842, -0.524, -0.253, 0, 0.253, 0.524, 0.842, 1.282, np.inf ]
#labels = np.arange(1, 11)
bins = [-np.inf , 0, np.inf ]
labels = np.arange(1, 3)

for attribute in list(pca_train.columns):
  if attribute != "HasDetections":
    train_df[attribute] = pd.cut(train_df[attribute], bins = bins, labels = labels)
    test_df[attribute] = pd.cut(test_df[attribute], bins = bins, labels = labels)

string_columns = train_df.select_dtypes(['object']).columns
le = LabelEncoder()
for col in string_columns :
  train_df[col] = le.fit_transform([str(i) for i in train_df[col].values])
  test_df[col] = le.fit_transform([str(i) for i in test_df[col].values])

#y_train = train_df['HasDetections']
#y_test = test_df['HasDetections']
#train_without_class = train_df
#test_without_class = test_df
#Drop HasDetections column
#train_without_class.drop(['HasDetections'], axis=1, inplace=True)
#test_without_class.drop(['HasDetections'], axis=1, inplace=True)

"""##Naive Bayes"""

pca_train = train_df.select_dtypes(['category'])
pca_test = test_df.select_dtypes(['category'])

#Set label lists for train and test data
y_train = train_df['HasDetections']
y_test = test_df['HasDetections']

#Drop HasDetections column
#pca_train.drop(['HasDetections'], axis=1, inplace=True)
#pca_test.drop(['HasDetections'], axis=1, inplace=True)

gnb = GaussianNB()
gnb.fit(pca_train, y_train )
prediction2 = gnb.predict(pca_test)

confusion_matrix(y_test.values, prediction2)

print(classification_report(y_test,prediction2))

"""https://www.kaggle.com/fabiendaniel/detecting-malwares-with-lgbm \\
^Good notebook to follow for encoding fields, may not require PCA

###LGBM
"""

for col in train_df.columns :
  train_df[col] = le.fit_transform([str(i) for i in train_df[col].values])
  test_df[col] = le.fit_transform([str(i) for i in test_df[col].values])

del train_df['HasDetections'], train_df['MachineIdentifier'], test_df['MachineIdentifier']

lgb_model = lgb.LGBMClassifier(max_depth=-1,
                                   n_estimators=30000,
                                   learning_rate=0.05,
                                   num_leaves=2 ** 12 - 1,
                                   colsample_bytree=0.28,
                                   objective='binary',
                                   n_jobs=-1,
                                   cat_l2=50,
                                   cat_smooth=0)

# Commented out IPython magic to ensure Python compatibility.
#lgb_model.fit(train_df, y_train,
#                  verbose=100)



m = RandomForestClassifier(n_estimators=100, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=False)
# %time m.fit(train_df, y_train)

"""###Decision Tree"""

y_train = train_df['HasDetections']
x_train = train_df.select_dtypes(['category'])
#x_train.drop(['HasDetections'], axis=1, inplace=True)

y_test = test_df['HasDetections']
x_test = test_df.select_dtypes(['category'])
#x_test.drop(['HasDetections'], axis=1, inplace=True)

clf = DecisionTreeClassifier()
clf = clf.fit(x_train, y_train)

prediction3 = clf.predict(x_test)

confusion_matrix(y_test.values, prediction3)

print(classification_report(y_test,prediction3))

"""##Ensemble"""

predictions = []
for i in range(len(prediction1)):
  if 0.6*prediction1[i] + 0.58*prediction2[i] + 0.54*prediction3[i] > 0.865:
    predictions.append(1)
  else:
    predictions.append(0)

confusion_matrix(y_test.values, predictions)

print(classification_report(y_test,predictions))

metrics.roc_auc_score(y_test, prediction1)